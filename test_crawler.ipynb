{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目前正在搜尋股票: 2330台積電 在Google的新聞清單  進度: 1 / 3\n",
      "目前正在下載: 2330台積電 各家新聞  進度: 1 / 11\n",
      "目前正在下載: 2330台積電 各家新聞  進度: 2 / 11\n",
      "目前正在下載: 2330台積電 各家新聞  進度: 3 / 11\n",
      "目前正在下載: 2330台積電 各家新聞  進度: 4 / 11\n",
      "目前正在下載: 2330台積電 各家新聞  進度: 5 / 11\n",
      "目前正在下載: 2330台積電 各家新聞  進度: 6 / 11\n",
      "目前正在下載: 2330台積電 各家新聞  進度: 7 / 11\n",
      "目前正在下載: 2330台積電 各家新聞  進度: 8 / 11\n",
      "目前正在下載: 2330台積電 各家新聞  進度: 9 / 11\n",
      "目前正在下載: 2330台積電 各家新聞  進度: 10 / 11\n",
      "目前正在下載: 2330台積電 各家新聞  進度: 11 / 11\n",
      "目前正在搜尋股票: 2317鴻海 在Google的新聞清單  進度: 2 / 3\n",
      "目前正在下載: 2317鴻海 各家新聞  進度: 1 / 13\n",
      "目前正在下載: 2317鴻海 各家新聞  進度: 2 / 13\n",
      "目前正在下載: 2317鴻海 各家新聞  進度: 3 / 13\n",
      "目前正在下載: 2317鴻海 各家新聞  進度: 4 / 13\n",
      "目前正在下載: 2317鴻海 各家新聞  進度: 5 / 13\n",
      "目前正在下載: 2317鴻海 各家新聞  進度: 6 / 13\n",
      "目前正在下載: 2317鴻海 各家新聞  進度: 7 / 13\n",
      "目前正在下載: 2317鴻海 各家新聞  進度: 8 / 13\n",
      "目前正在下載: 2317鴻海 各家新聞  進度: 9 / 13\n",
      "目前正在下載: 2317鴻海 各家新聞  進度: 10 / 13\n",
      "目前正在下載: 2317鴻海 各家新聞  進度: 11 / 13\n",
      "目前正在下載: 2317鴻海 各家新聞  進度: 12 / 13\n",
      "目前正在下載: 2317鴻海 各家新聞  進度: 13 / 13\n",
      "目前正在搜尋股票: 2412中華電 在Google的新聞清單  進度: 3 / 3\n",
      "目前正在下載: 2412中華電 各家新聞  進度: 1 / 4\n",
      "目前正在下載: 2412中華電 各家新聞  進度: 2 / 4\n",
      "目前正在下載: 2412中華電 各家新聞  進度: 3 / 4\n",
      "目前正在下載: 2412中華電 各家新聞  進度: 4 / 4\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Google新聞即時爬蟲\n",
    "程式碼撰寫: 蘇彥庭\n",
    "日期: 20210111\n",
    "\n",
    "程式修改日期: 2023/04/08\n",
    "1. 處理Google RSS連結: 原本為新聞連結 現在被改為Google頁面連結 連結該Google頁面後才會被轉向實際新聞連結\n",
    "2. 修改經濟日報新聞內容抓取方式\n",
    "\"\"\"\n",
    "\n",
    "# 載入套件\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "# 參數設定\n",
    "# 欲下載新聞的股票關鍵字清單\n",
    "searchList = ['2330台積電', '2317鴻海', '2412中華電']\n",
    "# 新聞下載起始日\n",
    "nearStartDate = (datetime.date.today() + datetime.timedelta(days=-10)).strftime('%Y-%m-%d')\n",
    "\n",
    "\n",
    "# 整理Google新聞資料用\n",
    "def arrangeGoogleNews(elem):\n",
    "    return ([elem.find('title').getText(),\n",
    "             elem.find('link').getText(),\n",
    "             elem.find('pubDate').getText(),\n",
    "             BeautifulSoup(elem.find('description').getText(), 'html.parser').find('a').getText(),\n",
    "             elem.find('source').getText()])\n",
    "\n",
    "\n",
    "# 擷取各家新聞網站新聞函數\n",
    "def beautifulSoupNews(url):\n",
    "\n",
    "    # 設定hearers\n",
    "    headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                             'Chrome/87.0.4280.141 Safari/537.36'}\n",
    "\n",
    "    # 取得Google跳轉頁面的新聞連結\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    newsUrl = soup.find_all('c-wiz', class_='jtabgf')[0].getText()\n",
    "    newsUrl = newsUrl.replace('Opening ', '')\n",
    "\n",
    "    # 取得該篇新聞連結內容\n",
    "    response = requests.get(newsUrl, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser') \n",
    "\n",
    "    # 判斷url網域做對應文章擷取\n",
    "    domain = re.findall('https://[^/]*', newsUrl)[0].replace('https://', '')\n",
    "\n",
    "    if domain == 'udn.com':\n",
    "\n",
    "        # 聯合新聞網\n",
    "        item = soup.find_all('section', class_='article-content__editor')[0].find_all('p')\n",
    "        content = [elem.getText() for elem in item]\n",
    "        content = ''.join(content)\n",
    "        content = content.replace('\\r', ' ').replace('\\n', ' ')\n",
    "\n",
    "    elif domain == 'ec.ltn.com.tw':\n",
    "\n",
    "        # 自由財經\n",
    "        item = soup.find_all('div', class_='text')[0].find_all('p', class_='')\n",
    "        content = [elem.getText() for elem in item]\n",
    "        content = ''.join(content)\n",
    "        content = content.replace('\\r', ' ').replace('\\n', ' ').replace(u'\\xa0', ' '). \\\n",
    "            replace('一手掌握經濟脈動', '').replace('點我訂閱自由財經Youtube頻道', '')\n",
    "\n",
    "    elif domain in ['tw.stock.yahoo.com', 'tw.news.yahoo.com']:\n",
    "\n",
    "        # Yahoo奇摩股市\n",
    "        item = soup.find_all('div', class_='caas-body')[0].find_all('p')\n",
    "        content = [elem.getText() for elem in item]\n",
    "        del_text = soup.find_all('div', class_='caas-body')[0].find_all('a')\n",
    "        del_text = [elem.getText() for elem in del_text]\n",
    "        content = [elem for elem in content if elem not in del_text]\n",
    "        content = ''.join(content)\n",
    "        content = content.replace('\\r', ' ').replace('\\n', ' ').replace(u'\\xa0', ' ')\n",
    "\n",
    "    elif domain == 'money.udn.com':\n",
    "\n",
    "        # 經濟日報\n",
    "        item = soup.find_all('section', id='article_body')[0].find_all('p')\n",
    "        content = [elem.getText() for elem in item]\n",
    "        content = [elem for elem in content]\n",
    "        content = ''.join(content)\n",
    "        content = content.replace('\\r', ' ').replace('\\n', ' ')\n",
    "\n",
    "    elif domain == 'www.chinatimes.com':\n",
    "\n",
    "        # 中時新聞網\n",
    "        item = soup.find_all('div', class_='article-body')[0].find_all('p')\n",
    "        content = [elem.getText() for elem in item]\n",
    "        content = [elem for elem in content]\n",
    "        content = ''.join(content)\n",
    "        content = content.replace('\\r', ' ').replace('\\n', ' ')\n",
    "\n",
    "    elif domain == 'ctee.com.tw':\n",
    "\n",
    "        # 工商時報\n",
    "        item = soup.find_all('div', class_='entry-content clearfix single-post-content')[0].find_all('p')\n",
    "        content = [elem.getText() for elem in item]\n",
    "        content = [elem for elem in content]\n",
    "        content = ''.join(content)\n",
    "        content = content.replace('\\r', ' ').replace('\\n', ' ')\n",
    "\n",
    "    elif domain == 'news.cnyes.com':\n",
    "\n",
    "        # 鉅亨網\n",
    "        item = soup.find_all('div', itemprop='articleBody')[0].find_all('p')\n",
    "        content = [elem.getText() for elem in item]\n",
    "        content = [elem for elem in content]\n",
    "        content = ''.join(content)\n",
    "        content = content.replace('\\r', ' ').replace('\\n', ' ').replace(u'\\xa0', ' ')\n",
    "\n",
    "    elif domain == 'finance.ettoday.net':\n",
    "\n",
    "        # ETtoday\n",
    "        item = soup.find_all('div', itemprop='articleBody')[0].find_all('p')\n",
    "        content = [elem.getText() for elem in item]\n",
    "        content = [elem for elem in content]\n",
    "        content = ''.join(content)\n",
    "        content = content.replace('\\r', ' ').replace('\\n', ' ').replace(u'\\xa0', ' ')\n",
    "\n",
    "    elif domain == 'fnc.ebc.net.tw':\n",
    "\n",
    "        # EBC東森財經新聞\n",
    "        content = str(soup.find_all('script')[-2]).split('ReactDOM.render(React.createElement(')[1]\n",
    "        content = content.split(',')[1].replace('{\"content\":\"', '').replace('\"})', '')\n",
    "        content = re.sub(u'\\\\\\\\u003[a-z]+', '', content)\n",
    "        content = content.replace('/p', ' ').replace('\\\\n', '')\n",
    "\n",
    "    else:\n",
    "\n",
    "        # 未知domain\n",
    "        content = 'unknow domain'\n",
    "\n",
    "    return newsUrl, content\n",
    "\n",
    "\n",
    "# 迴圈下載股票清單的Google新聞資料\n",
    "stockNews = pd.DataFrame()\n",
    "for iSearch in range(len(searchList)):\n",
    "\n",
    "    print('目前正在搜尋股票: ' + searchList[iSearch] +\n",
    "          ' 在Google的新聞清單  進度: ' + str(iSearch + 1) + ' / ' + str(len(searchList)))\n",
    "\n",
    "    # 建立搜尋網址\n",
    "    url = 'https://news.google.com/news/rss/search/section/q/' + \\\n",
    "          searchList[iSearch] + '/?hl=zh-tw&gl=TW&ned=zh-tw_tw'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'xml')\n",
    "    item = soup.find_all('item')\n",
    "    rows = [arrangeGoogleNews(elem) for elem in item]\n",
    "\n",
    "    # 組成pandas\n",
    "    df = pd.DataFrame(data=rows, columns=['title', 'link', 'pub_date', 'description', 'source'])\n",
    "    # 新增時間戳記欄位\n",
    "    df.insert(0, 'search_time', time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()), True)\n",
    "    # 新增搜尋字串\n",
    "    df.insert(1, 'search_key', searchList[iSearch], True)\n",
    "    # 篩選最近的新聞\n",
    "    df['pub_date'] = df['pub_date'].astype('datetime64[ns]')\n",
    "    df = df[df['pub_date'] >= nearStartDate]\n",
    "    # 按發布時間排序\n",
    "    df = df.sort_values(['pub_date']).reset_index(drop=True)\n",
    "\n",
    "    # 迴圈爬取新聞連結與內容\n",
    "    newsUrls = list()\n",
    "    contents = list()\n",
    "    for iLink in range(len(df['link'])):\n",
    "\n",
    "        print('目前正在下載: ' + searchList[iSearch] +\n",
    "              ' 各家新聞  進度: ' + str(iLink + 1) + ' / ' + str(len(df['link'])))\n",
    "\n",
    "        newsUrl, content = beautifulSoupNews(url=df['link'][iLink])\n",
    "        newsUrls.append(newsUrl)\n",
    "        contents.append(content)\n",
    "        time.sleep(3)\n",
    "\n",
    "    # 新增新聞連結與內容欄位\n",
    "    df['newsUrl'] = newsUrls\n",
    "    df['content'] = contents\n",
    "\n",
    "    # 儲存資料\n",
    "    stockNews = pd.concat([stockNews, df])\n",
    "\n",
    "# 輸出結果檢查\n",
    "stockNews.to_csv('checkData.csv', index=False, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
